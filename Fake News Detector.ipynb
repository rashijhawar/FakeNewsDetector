{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Fake News Detector**"},{"metadata":{},"cell_type":"markdown","source":"## Importing modules"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport os\nimport spacy\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.metrics import accuracy_score \nfrom sklearn.metrics import classification_report ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading the Dataset"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"real = pd.read_csv(\"/kaggle/input/fake-and-real-news-dataset/True.csv\")\nfake = pd.read_csv(\"/kaggle/input/fake-and-real-news-dataset/Fake.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"real.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fake.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Adding a label that indicates whether the news is fake or real.\n**Real news is denoted using 1.\nFake news is denoted using 0.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"real['label'] = 1\nfake['label'] = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Combining the two datasets and shuffling it."},{"metadata":{"trusted":true},"cell_type":"code","source":"news = pd.concat([real,fake])\nnews = news.sample(frac=1, random_state = 1).reset_index(drop = True)\nnews.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"news.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that our dataset has no null values."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(news['label']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above plot, we can see that our dataset is balanced."},{"metadata":{"trusted":true},"cell_type":"code","source":"news['text'] = news['title'] + ' ' + news['text']\nnews.drop(['title', 'subject','date'], inplace = True, axis = 1)\nnews.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Text Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"Now, we are removing stop words, punctutations, and digits from the news text, and are performing Lemmatization."},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = spacy.load('en')\n\ndef clean_text(text):\n    doc = nlp(text)\n    clean = []\n    for token in doc:\n        if not token.is_punct and not token.is_space and not token.is_digit:\n            if not token.is_stop:\n                clean.append(token.lemma_.lower())\n    return(' '.join(clean))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"news['text'] = news['text'].apply(clean_text)\nnews.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Word Clouds"},{"metadata":{},"cell_type":"markdown","source":"### ***Word Cloud for the Fake news.***"},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud(width = 800, height = 800, max_font_size = 120).generate(\" \".join(news[news['label'] == 0].text)) \n  \n# plot the word cloud for fake news data                      \nplt.figure(figsize = (8, 8)) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout() \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ***Word Cloud for the Real news.***"},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud(width = 800, height = 800, max_words=200, max_font_size = 120).generate(\" \".join(news[news['label'] == 1].text)) \n  \n# plot the word cloud for fake news data                      \nplt.figure(figsize = (8, 8)) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout() \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Splitting the data for training and testing."},{"metadata":{"trusted":true},"cell_type":"code","source":"text_train, text_test, label_train, label_test = train_test_split(news['text'], news['label'], test_size = 0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **CountVectorizer and TF-IDF Transformer.**"},{"metadata":{},"cell_type":"markdown","source":"We fit the CountVectorizer and TF-IDF Transformer on the training dataset, and transform on both training and testing dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"count_vectorizer = CountVectorizer()\nfreq_term_matrix = count_vectorizer.fit_transform(text_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf = TfidfTransformer()\ntfidf_matrix = tfidf.fit_transform(freq_term_matrix)\nprint(tfidf_matrix.toarray().shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_count_vect = count_vectorizer.transform(text_test)\ntest_tfidf = tfidf.transform(test_count_vect)\nprint(test_tfidf.toarray().shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Classification**"},{"metadata":{},"cell_type":"markdown","source":"### ***Logistic Regression Model***"},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg = LogisticRegression(C=1e4, max_iter=300)\nlogreg.fit(tfidf_matrix,label_train)\npredlogreg = logreg.predict(test_tfidf)\n\ncm = confusion_matrix(label_test, predlogreg)\nprint('Confusion Matrix:')\nprint(cm) \nprint('Accuracy Score:', accuracy_score(label_test, predlogreg))\nprint('Report: ')\nprint(classification_report(label_test, predlogreg)) \nprint(logreg.score(test_tfidf,label_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ***Naive Bayes Model***"},{"metadata":{"trusted":true},"cell_type":"code","source":"nb = MultinomialNB()\nnb.fit(tfidf_matrix,label_train)\nprednb = nb.predict(test_tfidf)\n\ncm = confusion_matrix(label_test, prednb)\nprint('Confusion Matrix:')\nprint(cm) \nprint('Accuracy Score:',accuracy_score(label_test, prednb))\nprint('Report: ')\nprint(classification_report(label_test, prednb))\nprint(nb.score(test_tfidf,label_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We see that the Logistic Regression model performs better."}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}